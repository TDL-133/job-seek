# M√©thodologie - Scraper Multi-Sources Parallel.ai + Firecrawl

## üìã Objectif

Script de test pour valider l'approche hybride de scraping d'offres d'emploi combinant :
- **Parallel.ai Search API** : Recherche intelligente d'URLs (Glassdoor + WTTJ)
- **Firecrawl Search API** : Recherche sp√©cialis√©e Indeed avec anti-bot bypass
- **Parallel.ai Extract API** : Extraction structur√©e du contenu
- **Tavily Search API** : Recherche compl√©mentaire optionnelle

**Use Case Test** : Trouver 3 offres "Product Manager" √† Bordeaux par source (Glassdoor, WTTJ, Indeed) = 9 offres cible.

---

## üèóÔ∏è Architecture en 3 Phases

### Phase 1 : Multi-Source Search avec Filtrage
**Objectif** : Trouver les URLs des offres d'emploi r√©elles (pas de pages agr√©g√©es)

**M√©thodes** :
1. **Parallel Search API** (prioritaire)
   - Endpoint : `POST https://api.parallel.ai/v1beta/search`
   - Mode : `agentic` (optimis√© pour recherche intelligente)
   - Queries cibl√©es : `site:glassdoor.com Product Manager Bordeaux`
   - Retour : Liste d'URLs + excerpts pertinents

2. **Tavily Search MCP** (compl√©mentaire)
   - Via tool MCP : `tavily-search`
   - Sp√©cialis√© dans r√©sultats de recherche pr√©cis
   - Query optimis√©e : `"Product Manager Bordeaux (site:glassdoor.com OR site:welcometothejungle.com) job posting"`
   - Retour : URLs + score de pertinence

3. **Firecrawl Search MCP** (pr√©par√© pour futur)
   - Via tool MCP : `firecrawl_search`
   - Scraping avec anti-bot bypass int√©gr√©
   - Retour : URLs + markdown content

**Filtrage des URLs** :
Exclut automatiquement :
- Pages Glassdoor : `/Salaries/`, `/Overview/`, `/Reviews/`, `/Emploi/`, `-jobs-SRCH`
- Pages WTTJ : `/companies/` sans `/jobs/`
- Pages g√©n√©riques : `/search`, `/categories`

Accepte uniquement :
- Glassdoor : `/job-listing/`, `/partner/`, URLs de postes individuels
- WTTJ : URLs contenant `/jobs/[job-slug]`

**Output** : Liste d'URLs filtr√©es et d√©dupliqu√©es, limit√©e √† 3 par source (Glassdoor + WTTJ)

---

### Phase 2 : Content Extraction
**Objectif** : Extraire le contenu complet de chaque offre

**M√©thode** :
- **Parallel Extract API**
  - Endpoint : `POST https://api.parallel.ai/v1beta/extract`
  - Input : Toutes les URLs trouv√©es en Phase 1
  - Objective : Extraire titre, entreprise, localisation, salaire, type de contrat, remote, description, comp√©tences, date
  - Options : `excerpts: true` + `full_content: true`

**Output** : JSON structur√© avec excerpts + full_content par URL

---

### Phase 3 : Structuring & Export
**Objectif** : Parser, normaliser et exporter en CSV

**Parsing intelligent** :
- **Regex patterns** pour extraire :
  - Salaire : `‚Ç¨XX,XXX - ‚Ç¨XX,XXX` ou `XXk - XXk`
  - Date : formats `DD/MM/YYYY`, `DD MMM YYYY`
  - Type de contrat : CDI, CDD, Stage, Alternance, Freelance
  - Remote : Remote, Hybrid, Onsite (mots-cl√©s FR/EN)
- **Keyword matching** pour comp√©tences (agile, scrum, jira, sql, figma, etc.)

**Output** : CSV avec 12 colonnes + JSON pour debug

---

## üìä Sch√©ma de Donn√©es

### Colonnes CSV finales

| Colonne | Type | Description | Exemple |
|---------|------|-------------|---------|
| `title` | String (requis) | Titre du poste | Product Manager |
| `company` | String | Nom de l'entreprise | Acme Corp |
| `location` | String | Ville/r√©gion | Bordeaux, France |
| `salary` | String | Fourchette salariale | 50k - 70k EUR |
| `contract_type` | String | Type de contrat | CDI, CDD, Stage |
| `remote` | String | Politique t√©l√©travail | Remote, Hybrid, Onsite |
| `description` | String | Description compl√®te (max 500 chars) | Nous recherchons... |
| `skills` | String | Comp√©tences requises (comma-separated) | agile, scrum, jira |
| `posted_date` | String | Date de publication | 15/11/2024 |
| `url` | String (requis) | URL de l'offre | https://... |
| `source` | String (requis) | Plateforme source | glassdoor, wttj |
| `extraction_method` | String | M√©thode d'extraction | parallel_extract |

---

## üîë APIs Utilis√©es

### 1. Parallel.ai Search API
```bash
POST https://api.parallel.ai/v1beta/search
Headers:
  x-api-key: TDevMkqIQNpuo5aTwTn5FAJ9BcKRpSk394Otl5pv
  parallel-beta: search-extract-2025-10-10
  Content-Type: application/json

Body:
{
  "mode": "agentic",
  "objective": "Find Product Manager job postings in Bordeaux, France...",
  "search_queries": [
    "site:glassdoor.com Product Manager Bordeaux",
    "site:welcometothejungle.com Product Manager Bordeaux"
  ],
  "max_results": 6,
  "excerpts": {"max_chars_per_result": 500}
}
```

**Retour** :
```json
{
  "search_id": "search_...",
  "results": [
    {
      "url": "https://www.glassdoor.com/job/...",
      "title": "Product Manager - Bordeaux",
      "excerpts": ["Excerpt 1", "Excerpt 2"],
      "publish_date": "2024-11-15"
    }
  ]
}
```

### 2. Parallel.ai Extract API
```bash
POST https://api.parallel.ai/v1beta/extract
Headers: (same as Search)

Body:
{
  "urls": ["https://...", "https://..."],
  "objective": "Extract job posting details: title, company, location, salary...",
  "excerpts": true,
  "full_content": true
}
```

**Retour** :
```json
{
  "extract_id": "extract_...",
  "results": [
    {
      "url": "https://...",
      "title": "Product Manager",
      "excerpts": ["Relevant excerpt 1", "..."],
      "full_content": "Full page content..."
    }
  ]
}
```

### 3. Firecrawl Search (MCP Tool)
```python
# Via MCP tool call (non impl√©ment√© dans ce test)
call_mcp_tool(
  name="firecrawl_search",
  input={
    "query": "Product Manager Bordeaux site:glassdoor.com",
    "limit": 3,
    "sources": [{"type": "web"}]
  }
)
```

---

## üöÄ Utilisation

### Installation des d√©pendances
```bash
# httpx est d√©j√† install√© dans le projet principal
cd /Users/lopato/Documents/DAGORSEY/Geek/Job\ Seek
source venv/bin/activate  # Si environnement virtuel
```

### Ex√©cution du script
```bash
cd scripts/job_scraping_test
python parallel_scraper.py
```

### Param√®tres modifiables
Dans `parallel_scraper.py`, fonction `main()` :
```python
KEYWORDS = "Product Manager"       # Mots-cl√©s de recherche
LOCATION = "Bordeaux"             # Ville
LIMIT_PER_SOURCE = 3              # Nombre d'offres par plateforme
```

---

## üìÅ Outputs G√©n√©r√©s

Tous les fichiers sont cr√©√©s dans `results/` :

1. **`parallel_search.json`** : R√©ponse brute de Parallel Search API
2. **`parallel_extract.json`** : R√©ponse brute de Parallel Extract API
3. **`jobs.json`** : Jobs structur√©s en JSON (debug)
4. **`jobs.csv`** : ‚≠ê **CSV final lisible** avec toutes les colonnes

### Exemple de sortie console
```
üöÄ Starting job search: 'Product Manager' in 'Bordeaux'
üìç Limit: 3 jobs per source (Glassdoor + WTTJ)
======================================================================

üì° PHASE 1: Multi-Source Search
   üîé Running Parallel Search API...
   ‚úì Parallel Search found 8 URLs

‚úÖ Found 6 unique job URLs:
   - Glassdoor: 3
   - WTTJ: 3

üîç PHASE 2: Content Extraction
   üîç Extracting content from 6 URLs...
   ‚úì Extracted 6 pages

üìä PHASE 3: Data Structuring & Export
   ‚úì Structured: Product Manager @ Acme Corp
   ‚úì Structured: Senior Product Manager @ Beta Inc
   ...
   ‚úì CSV exported: results/jobs.csv

======================================================================
‚úÖ SUCCESS! Found 6 complete job postings
üìÑ CSV: results/jobs.csv
üìÅ JSON files saved in: results/
```

---

## üìà R√©sultats des Tests

### Test initial (avant filtrage)
- **URLs trouv√©es** : 6 URLs
- **Offres r√©elles** : 2 WTTJ (33% de pr√©cision)
- **URLs filtr√©es** : 4 (pages Glassdoor non pertinentes)
  - 1x `/Salaries/` (donn√©es salariales)
  - 1x `/Overview/` (pr√©sentation entreprise)
  - 1x `/Emploi/` (liste de recherche)
  - 1x autre (page agr√©g√©e)

### Test am√©lior√© v1 (avec filtrage strict)
- **URLs brutes** : 9 URLs de Parallel Search
- **URLs filtr√©es** : 5 URLs non pertinentes exclues
- **Offres finales** : 0 Glassdoor + 3 WTTJ
- **Probl√®me** : Filtrage trop strict excluait pages de r√©sultats Glassdoor

### Test optimis√© v2 (avec filtrage assoupli + max_results x5)
- **URLs brutes** : 15 URLs de Parallel Search
- **URLs filtr√©es** : 3 URLs non pertinentes exclues
- **URLs Glassdoor accept√©es** : 2 pages de r√©sultats (`/Job/...SRCH_IL` et `/Emploi/...SRCH_IL`)
- **Offres finales** : **2 Glassdoor + 3 WTTJ = 5 offres**
- **Am√©lioration** : Passage de 0 ‚Üí 2 Glassdoor (‚àû% de croissance)
- **Pr√©cision globale** : 83% (5 offres sur 6 URLs finales)

**Note sur Tavily** : Tavily API REST impl√©ment√©e mais n√©cessite `TAVILY_API_KEY` dans `.env`. Free tier disponible : 1000 recherches/mois sur https://tavily.com/

## ‚ö†Ô∏è Consid√©rations Techniques

### Gestion des erreurs
- Try/except sur chaque API call
- Continue si une source √©choue
- Logging d√©taill√© des erreurs et URLs filtr√©es
- Minimum 1 offre requise pour succ√®s

### Performance
- **Phase 1** : ~5-10s (2 requ√™tes en parall√®le + filtrage)
- **Phase 2** : ~10-15s (1 requ√™te batch pour toutes les URLs)
- **Phase 3** : ~1s (processing local)
- **Total** : ~20-30s pour 3-6 offres

### Limites
- Tavily API REST impl√©ment√©e mais non activ√©e (cl√© API requise)
- Parsing heuristique (peut manquer certains champs)
- Limite de 3 offres par source dans ce test
- Pages de r√©sultats Glassdoor contiennent plusieurs offres mais trait√©es comme 1 seule offre
- Patterns de filtrage √† maintenir si structures d'URLs changent

### Optimisations Glassdoor appliqu√©es
**Patterns accept√©s** :
- `/job-listing/[job-id]` : Offres individuelles
- `/partner/jobListing.htm` : Offres partenaires
- `/Job/[location]-[title]-jobs-SRCH_IL...` : **Pages de r√©sultats localis√©es** (NOUVEAU)
- `/Emploi/[location]-[title]-emplois-SRCH_IL...` : **Version fran√ßaise** (NOUVEAU)

**Patterns rejet√©s** :
- `/Salaries/` : Donn√©es salariales
- `/Overview/` : Pages entreprise
- `/Reviews/` : Avis employ√©s
- `/Interview/` : Questions d'entretien
- `/Job/...SRCH_IN...` : Recherches globales (non localis√©es)

**Cl√©** : Accepter `SRCH_IL` (localized) mais rejeter `SRCH_IN` (global)

---

## üîÑ Prochaines √âtapes

Si le test est concluant :

1. **Int√©gration dans l'app principale**
   - Remplacer les scrapers BeautifulSoup existants
   - Utiliser ce workflow dans `JobSearchService`

2. **Augmentation des limites**
   - Passer √† 50+ offres par source
   - Ajouter pagination si n√©cessaire

3. **Ajout d'autres sources**
   - Indeed (via Unipile ou Parallel)
   - LinkedIn (via Unipile existant)

4. **Am√©lioration du parsing**
   - Utiliser Claude API pour extraction structur√©e
   - Ajouter validation des champs

---

## üìö R√©f√©rences

- **Parallel.ai Docs** : https://docs.parallel.ai/
- **Firecrawl Docs** : https://docs.firecrawl.dev/
- **Plan complet** : Voir `/docs/` ou Warp Drive Notebook

---

## ‚ú® Ajout Indeed avec Firecrawl Search (Version 3)

### M√©thode
**API utilis√©e** : Firecrawl Search API (`POST https://api.firecrawl.dev/v1/search`)

**Pourquoi Firecrawl uniquement pour Indeed ?**
- Indeed a des protections anti-scraping tr√®s puissantes
- Firecrawl est sp√©cialis√© dans le bypass d'anti-bot
- Plus fiable que Parallel Search pour Indeed
- Retourne directement URLs + markdown content

**Configuration** :
```bash
# Dans .env
FIRECRAWL_API_KEY=fc-...
```

**Impl√©mentation** :
```python
# M√©thode _firecrawl_search_indeed() dans parallel_scraper.py
async def _firecrawl_search_indeed(keywords, location, max_results):
    response = await httpx.post(
        "https://api.firecrawl.dev/v1/search",
        headers={"Authorization": f"Bearer {FIRECRAWL_API_KEY}"},
        json={
            "query": f"{keywords} {location} site:fr.indeed.com",
            "limit": max_results,
            "scrapeOptions": {"formats": ["markdown"]}
        }
    )
```

### Patterns Indeed
**URLs accept√©es** :
- ‚úÖ `/viewjob?jk=` : Offres individuelles (format principal)
- ‚úÖ `/rc/clk?jk=` : Liens de redirection vers offres
- ‚úÖ `/cmp/[company]/jobs/[title]` : Offres h√©berg√©es par entreprise

**URLs rejet√©es** :
- ‚ùå `/jobs?q=` : Pages de r√©sultats de recherche
- ‚ùå `/companies/` : Pages entreprises sans offre
- ‚ùå `/career-advice/` : Articles de conseil
- ‚ùå `/salaries/` : Pages de donn√©es salariales

### R√©sultats Test v3
**Ex√©cution** : `python parallel_scraper.py`

**Phase 1 - URLs brutes** :
- Parallel Search (Glassdoor + WTTJ) : 15 URLs
- Firecrawl Search (Indeed) : 9 URLs
- **Total** : 24 URLs brutes

**Phase 1 - Apr√®s filtrage** :
- Glassdoor : 2 URLs (pages r√©sultats localis√©es `SRCH_IL`)
- WTTJ : 1 URL (offre individuelle `/jobs/`)
- Indeed : 3 URLs (offres `/viewjob?jk=`)
- **Total** : 6 URLs filtr√©es

**Phase 2 & 3 - Extraction & Export** :
- 6 pages extraites via Parallel Extract API
- 6 offres structur√©es dans `results/jobs.csv`
- Colonne `source` : `glassdoor`, `wttj`, **`indeed`** ‚úì

**Performance** :
- Phase 1 : ~15s (Parallel 5s + Firecrawl 10s)
- Phase 2 : ~15s (Extraction)
- **Total** : ~30s pour 6 offres

### Am√©liorations futures
- Augmenter `limit_per_source` pour plus d'offres par source
- Am√©liorer parsing avec Claude API
- Optimiser filtrage URL pour Indeed (pages de r√©sultats)

---

## ‚ú® Ajout LinkedIn avec Unipile API (Version 4)

### M√©thode
**API utilis√©e** : Unipile LinkedIn Search API (`POST {DSN}/api/v1/linkedin/search`)

**Pourquoi Unipile uniquement pour LinkedIn ?**
- API officielle avec authentification LinkedIn r√©elle
- Pas de scraping web (pas de blocage)
- Acc√®s direct aux donn√©es structur√©es
- Rate limits : ~1000 recherches/jour pour LinkedIn
- Plus fiable que web scraping ou Parallel Search

**Configuration** :
```bash
# Dans .env (d√©j√† configur√©)
UNIPILE_DSN=https://api21.unipile.com:15160
UNIPILE_API_KEY=85adQehB.dm6vrV/Wf/JY9/ClN2EZbWDhKg5RjTpHbZbOGm/xQxU=
UNIPILE_LINKEDIN_ACCOUNT_ID=6ariH5hYQf2Kq6UhLVG6UQ
```

**Impl√©mentation** :
```python
# M√©thode _unipile_search_linkedin() dans parallel_scraper.py
async def _unipile_search_linkedin(keywords, location, max_results):
    response = await httpx.post(
        f"{unipile_dsn}/api/v1/linkedin/search",
        headers={"X-API-KEY": unipile_api_key},
        params={"account_id": unipile_account_id},
        json={
            "api": "classic",
            "category": "jobs",
            "keywords": f"{keywords} {location}"
        }
    )
    # Parse items array, filter type="JOB" + location
    for item in data.get("items", []):
        if item.get("type") == "JOB":
            # Filter by location (important!)
            item_location = item.get("location", "").lower()
            if location and location.lower() not in item_location:
                continue  # Skip jobs not in specified location
            urls.append(item.get("job_url"))
```

### Patterns LinkedIn
**URLs accept√©es** :
- ‚úÖ `/jobs/view/` : Offres individuelles (format principal)
- ‚úÖ Toutes les URLs retourn√©es par Unipile sont valides (d√©j√† filtr√©es par type="JOB")

**URLs rejet√©es** :
- ‚ùå `/jobs/search/` : Pages de recherche
- ‚ùå `/company/` : Pages entreprises

### R√©sultats Test v4
**Ex√©cution** : `python parallel_scraper.py`

**Phase 1 - URLs brutes** :
- Parallel Search (Glassdoor + WTTJ) : 15 URLs
- Firecrawl Search (Indeed) : 9 URLs
- Unipile LinkedIn Search : **10 URLs**
- **Total** : 34 URLs brutes

**Phase 1 - Apr√®s filtrage** :
- Glassdoor : 2 URLs
- WTTJ : 3 URLs
- Indeed : 3 URLs
- LinkedIn : **3 URLs**
- **Total** : 11 URLs filtr√©es

**Phase 2 & 3 - Extraction & Export** :
- 11 pages extraites via Parallel Extract API
- 11 offres structur√©es dans `results/jobs.csv`
- Colonne `source` : `glassdoor`, `wttj`, `indeed`, **`linkedin`** ‚úì

**Performance** :
- Phase 1 : ~20s (Parallel 5s + Firecrawl 10s + Unipile 5s)
- Phase 2 : ~15s (Extraction)
- **Total** : ~35s pour 11 offres

**Distribution finale** :
- üîµ Glassdoor : 2 offres
- üü¢ WTTJ : 3 offres
- üî¥ Indeed : 3 offres
- üîµ LinkedIn : **3 offres**
- **Total** : **11 offres sur 4 sources actives**

### Avantages Unipile
- Authentification r√©elle LinkedIn (compte Julien Lopato)
- Donn√©es structur√©es (titre, entreprise, localisation, remote, description)
- Pas de parsing HTML n√©cessaire
- Pas de blocage anti-bot
- Rate limits raisonnables (~1000/jour)
- **Filtrage g√©ographique** : Les r√©sultats sont filtr√©s c√¥t√© script pour ne garder que les offres dont la localisation contient "bordeaux"

### Am√©lioration : Filtrage g√©ographique LinkedIn
**Probl√®me initial** : Unipile retourne 10 jobs dont seulement 3 √† Bordeaux, mais le script prenait les 3 premiers (Paris, France remote, Bordeaux)

**Solution** : Ajout d'un filtre post-API dans `_unipile_search_linkedin()` :
```python
item_location = item.get("location", "").lower()
if location and location.lower() not in item_location:
    continue  # Skip jobs not in specified location
```

**R√©sultat** : Les 3 offres LinkedIn sont maintenant toutes √† Bordeaux (√©tait Paris/Marseille avant)

---

**Date de cr√©ation** : 28 novembre 2024  
**Derni√®re mise √† jour** : 29 novembre 2024 (ajout Indeed + LinkedIn + filtrage g√©o LinkedIn)  
**Auteur** : Job Seek Team  
**Version** : 1.4.1 (Test avec 4 sources + filtrage g√©ographique)
